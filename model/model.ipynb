{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone as PineconeClient\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import Pinecone as PineconeLang\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"../data/imdb_movie_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rank', 'Title', 'Genre', 'Description', 'Director', 'Actors', 'Year',\n",
       "       'Runtime (Minutes)', 'Rating', 'Votes', 'Revenue (Millions)',\n",
       "       'Metascore'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will make it easier for us to process the data that way we have all the information in one column\n",
    "def create_movie_chunk(row):\n",
    "    chunk = f\"\"\"\n",
    "    Title: {row['Title']}\n",
    "    Genre: {row['Genre']}\n",
    "    Director: {row['Director']}\n",
    "    Actors: {row['Actors']}\n",
    "    Description: {row['Description']}\n",
    "    Year: {row['Year']}\n",
    "    Rating: {row['Rating']}/10\n",
    "    Metascore: {row['Metascore']}/100\n",
    "    \"\"\"\n",
    "    return chunk.strip()  # Remove leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Guardians of the Galaxy\n",
      "    Genre: Action,Adventure,Sci-Fi\n",
      "    Director: James Gunn\n",
      "    Actors: Chris Pratt, Vin Diesel, Bradley Cooper, Zoe Saldana\n",
      "    Description: A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.\n",
      "    Year: 2014\n",
      "    Rating: 8.1/10\n",
      "    Metascore: 76.0/100\n"
     ]
    }
   ],
   "source": [
    "# Apply to the dataframe\n",
    "df[\"chunk\"] = df.apply(create_movie_chunk, axis=1)\n",
    "\n",
    "# Preview a chunk, these will then each be transformed into a vector (each row  in the df its own vector)\n",
    "print(df[\"chunk\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Texts from our dataframe into one big chunk\n",
    "texts = df[\"chunk\"].tolist()  # Our structured movie chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m----> 3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-embedding-3-small\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:338\u001b[0m, in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/openai/_client.py:130\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/openai/_base_client.py:870\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/openai/_base_client.py:762\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/httpx/_client.py:688\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, auth, params, headers, cookies, verify, cert, trust_env, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, default_encoding)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/httpx/_client.py:731\u001b[0m, in \u001b[0;36m_init_transport\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/httpx/_transports/default.py:153\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/httpx/_config.py:40\u001b[0m, in \u001b[0;36mcreate_ssl_context\u001b[0;34m(verify, cert, trust_env)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:770\u001b[0m, in \u001b[0;36mcreate_default_context\u001b[0;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(purpose)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[0;32m--> 770\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mverify_mode \u001b[38;5;241m!=\u001b[39m CERT_NONE:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs(purpose)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_5jRVqy_KMSsgNvdoujgzDxFbHULRx6JiMScHN9t95KswjU7g7qdfdWmpeUa4S8E4sfsHn5\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "print(pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN AS VECTOR DATABASE HAS ALREADY BEEN CREATED AND POPULATED\n",
    "# This cell creates the indexes for each movie\n",
    "pc = PineconeClient(api_key=pinecone_api_key)\n",
    "index_name = \"lumaa\"\n",
    "docsearch = PineconeLang.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if pinecone indexes are already created and stored in our database (they have already been created for you.)\n",
    "pc = PineconeClient(api_key=pinecone_api_key)\n",
    "docsearch = PineconeLang.from_existing_index(index_name=\"lumaa\", embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "\n",
    "You are an expert movie recommender who can understand what the person is asking and what kind of movies they like based on the given information that they give you.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Return the helpful answer that gives them 5 total recommendations that they should look at. \n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binds our prompt to this varaible \"PROMPT that will be fed to the llm later\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"question_prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/0dl3xb3j0tz94x64f_ht6h5c0000gn/T/ipykernel_81820/3355871334.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # This key is used in the prompt template for conversation history.\n",
    "    return_messages=True         # Returns the chat history as a list of messages.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for openai",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This chain binds together both our llm and the vector database we made \u001b[39;00m\n\u001b[1;32m      7\u001b[0m qa \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[0;32m----> 8\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      9\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mdocsearch\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}),\n\u001b[1;32m     10\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[1;32m     11\u001b[0m     combine_docs_chain_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: PROMPT},\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     emit_warning()\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/pydantic/_internal/_decorators_v1.py:148\u001b[0m, in \u001b[0;36m_wrapper1\u001b[0;34m(values, _)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/langchain_core/utils/pydantic.py:218\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(cls, values)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:342\u001b[0m, in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/lumaa-spring-2025-ai-ml/lumaa_env/lib/python3.11/site-packages/langchain_community/utils/openai.py:10\u001b[0m, in \u001b[0;36mis_openai_v1\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:1009\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:982\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    977\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:565\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for openai"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This chain binds together both our llm and the vector database we made \n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=API_KEY),\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/0dl3xb3j0tz94x64f_ht6h5c0000gn/T/ipykernel_81820/2746102017.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your interest in romantic novels, I would recommend the following movies:\n",
      "\n",
      "1. \"The Notebook\" - Directed by Nick Cassavetes, this movie is a classic romantic drama based on Nicholas Sparks' novel of the same name.\n",
      "\n",
      "2. \"Pride and Prejudice\" - This is a beautiful adaptation of Jane Austen's classic novel, directed by Joe Wright.\n",
      "\n",
      "3. \"Bridget Jones's Diary\" - A romantic comedy based on Helen Fielding's novel, directed by Sharon Maguire.\n",
      "\n",
      "4. \"Sense and Sensibility\" - Another Jane Austen adaptation, this time directed by Ang Lee.\n",
      "\n",
      "5. \"Me Before You\" - A romantic drama based on Jojo Moyes' novel, directed by Thea Sharrock.\n",
      "\n",
      "These movies, like \"Love, Rosie\" and \"P.S. I Love You\", are all based on popular romantic novels and should be right up your alley.\n"
     ]
    }
   ],
   "source": [
    "query = \"I love romantic novels\"\n",
    "response = qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#Conversational Chain that highlights the use of the Conversational Buffer Memmory\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")  # Get user input\n",
    "\n",
    "    if query.lower() in [\"exit\", \"quit\", \"stop\"]:  # Allow user to exit\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = qa.run(query)  # Get response from the chatbot\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumaa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
